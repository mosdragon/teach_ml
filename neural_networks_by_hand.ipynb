{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Today, we'll explore [Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) in depth, a wide class of algorithms that _learn_ by mimicking the human brain. In particular, we're going to implement two types of neural networks.\n",
    "\n",
    "- [Perceptron](https://en.wikipedia.org/wiki/Perceptron)\n",
    "- [Feedfordward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network), specifically a [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "\n",
    "To better understand this tutorial, we'll need to understand the basics of Object-Oriented Programming in Python. Here's a basic [guide](https://realpython.com/python3-object-oriented-programming/).\n",
    "\n",
    "The most important parts to understand are:\n",
    "- What is a class?\n",
    "  - __ANS__: A _class_ is a _template_ for data.\n",
    "- What is an Object?\n",
    "  - __ANS__: When you use the template, you create an _object_.\n",
    "- How do we store attributes in the class for use later?\n",
    "  - __ANS__:  The _class_ definition has attributes creates in the `__init__()` function. These are variables stored separately for each _object_ created from the _class_\n",
    "- How do we call an instance method (function) from another instance method?\n",
    "  - __ANS__: From any other method, we simply say `self.function_name()`, where `self` is a special variable that means _\"this object itself\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP Example\n",
    "\n",
    "Here, we show an example of how to create a _class_ in Python. The class is called _Dog_, and it has two functions, `bark()` and `bark_twice()`, as well as two attributes, `name` and `breed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    \"\"\"\n",
    "    This is the class definition. Everything indented over one from\n",
    "    this point onward is part of the class definition for a dog.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        This is a special function used to initialize an object. We'll\n",
    "        see how it's called later, but just understand that \"self\" here\n",
    "        refers to \"this object\".\n",
    "        \n",
    "        In this initialization function, we do \"self.name\" and define it\n",
    "        to be the name variable passed in this init function.\n",
    "        \n",
    "        We can create as many fields as we want.\n",
    "        \"\"\"\n",
    "        # This field was passed into the init func\n",
    "        self.name = name\n",
    "        \n",
    "        # This field was not but we can still initialize it to something\n",
    "        self.breed = \"Unknown Doggo\"\n",
    "        \n",
    "    def bark(self):\n",
    "        # We can access this object's attributes by using\n",
    "        # self.attribute_name in our function\n",
    "        print('\"Woof\" - {}'.format(self.name))\n",
    "        \n",
    "    def bark_twice(self):\n",
    "        # We can also call other functions or \"instance methods\" on this\n",
    "        # object from other functions. Here, we'll just call the bark()\n",
    "        # function twice.\n",
    "        self.bark()\n",
    "        self.bark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Woof\" - Wilfred\n",
      "Wilfred barked once\n",
      "\"Woof\" - Wilfred\n",
      "\"Woof\" - Wilfred\n",
      "Wilfred barked twice\n"
     ]
    }
   ],
   "source": [
    "wilfred = Dog(\"Wilfred\")  # Notice here we only pass one variable, the name\n",
    "wilfred.bark() # Notice here we pass in no var, not even \"self\"\n",
    "print(\"Wilfred barked once\")\n",
    "wilfred.bark_twice() # Notice here we pass in no var, not even \"self\"\n",
    "print(\"Wilfred barked twice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Perceptron Classifier\n",
    "\n",
    "Now we know enough OOP to proceed to creating a Perceptron Binary Classifier.\n",
    "\n",
    "Here are the main ideas:\n",
    "- A single perceptron has a single output, a `0` or a `1`. For now let's say we're only trying to ever predict `positive` or `negative`\n",
    "- That perceptron also accepts as many inputs as your dataset requires. Ex: The X=(age, weight, height), y=(gender) dataset would be 3 features, so a Perceptron for that problem would have 3 weights\n",
    "- I lied, there is an additional weight, that is called the _bias_, so this example problem needs _4_ weights\n",
    "\n",
    "Here's effectively the formula being applied to get a `1` or `0` for a single `X` vector, $X_{i}$:\n",
    "\n",
    "$$\n",
    "\\text{prediction} = W_1 * X_{i, 1} + W_2 * X_{i, 2} + W_3 * X_{i, 3} + \\text{bias}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\sum{W_j * X_{i, j}}) + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{round(prediction)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X_{i, j}$ is the $j$-th element in vector $X_{i}$, or the $j$-th feature vector value\n",
    "- $W_{j}$ is the $j$-th element in vector $W$\n",
    "- $b$ is the _bias_ weight, also just called the _bias_\n",
    "- $\\text{prediction}$ is the number between `0` and `1` that comes out of this computation\n",
    "- $\\hat{y}$ is the actual class label prediction, which we get by rounding the $\\text{prediction}$, so we get either `0` or `1`, which would correspond to two possible class labels (e.g. `negative` is `0`, `positive` is `1`)\n",
    "\n",
    "\n",
    "## Training Perceptron\n",
    "To train your perceptron, you're trying to find weights $W$ (and also _bias_ $b$) so that this $\\hat{y}$ prediction matches the true $y$ for as much of your training set as possible.\n",
    "\n",
    "The way we do this is by defining an error function that we can find the [derivative](https://en.wikipedia.org/wiki/Derivative) function of (this is Calculus-based), then computing the derivative of that function using a specific datapoint (each $X_i$ inidividually), then updating the weights by a teeny bit.\n",
    "\n",
    "__NOTE:__ A _derivative_ is conceptually same as the slope of a line, but now applies to curves rather than just straight lines. It is a slope re-computed at every point along the curve.\n",
    "\n",
    "![Derivative Example](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tangent_to_a_curve.svg/400px-Tangent_to_a_curve.svg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for our random number generator.\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"\n",
    "    Perceptron classifer:\n",
    "\n",
    "    Params:\n",
    "    :learning_rate: float\n",
    "        Learning rate\n",
    "    :max_iter: int\n",
    "        Passes over the training dataset\n",
    "    :random_state: int\n",
    "        Random number generator seed for random weight initialization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, max_iter=50, random_state=seed):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.errors = []\n",
    "        self.bias = 0 # We'll change this later.\n",
    "\n",
    "    def dotprod(self, xi):\n",
    "        \"\"\"\n",
    "        Return the dot product of the input with the bias added.\n",
    "        \n",
    "        This computes the output of the perceptron itself, without rounding up or down\n",
    "        to make a label prediction. This is matrix multiplication. \n",
    "        \"\"\"\n",
    "        # This is equivalent to \"y-hat = Wx + b\" with x being a single example\n",
    "        # and y-hat being the prediction.\n",
    "        return np.dot(xi, self.weights) + self.bias\n",
    "\n",
    "    def predict(self, xi):\n",
    "        \"\"\"\n",
    "        Return class label 1 if above 0\n",
    "        Return class label -1 otherwise\n",
    "        \"\"\"\n",
    "        # This makes all positive results class label 1, otherwise class label -1\n",
    "        return np.where(self.dotprod(xi) >= 0.0, 1, -1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluated the percentage of predictions that are correct.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        total = X.shape[0]\n",
    "        correct = (predictions == y).sum()\n",
    "        # TODO: Return the percentage of predictions that are correct.\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create a random number generator from the random_state.\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # Initialize with random weights. They must be relatively small, so\n",
    "        # loc=0.0 and scale=0.01 set the mean and variance, and each number\n",
    "        # get pulled from a normal distribution with those params.\n",
    "        # We need (num_attributes) weights and then a separate one for the bias\n",
    "        num_attributes = X.shape[1]\n",
    "        self.weights = rgen.normal(loc=0.0, scale=0.01, size=num_attributes)\n",
    "        \n",
    "        # TODO: Initialize a size 1 random number just like the line above.\n",
    "        self.bias = 0\n",
    "\n",
    "        # This is where iterations come in.\n",
    "        for idx in range(self.max_iter):\n",
    "            differences = []  # The differences between predictions and actual vals\n",
    "            for xi, target in zip(X, y):\n",
    "                # Compute predictions for each instance.\n",
    "                output = self.dotprod(X)\n",
    "                \n",
    "                # Compute the difference between the target and prediction\n",
    "                difference = (target - self.predict(xi))\n",
    "\n",
    "                # TODO: The update is then the learning_rate times the difference\n",
    "                update = 0\n",
    "                \n",
    "                # All weights except the bias get modified with this formula\n",
    "                # The are modified by the product of the update (for a certain\n",
    "                # attribute) with the value of that attribute in this example\n",
    "                self.weights += update * xi  # This is element-by-element vector multiplication\n",
    "\n",
    "                # The bias unit doesn't have an feature associated with it, so it's updated directly\n",
    "                self.bias += update\n",
    "\n",
    "                differences.append(difference)\n",
    "\n",
    "            differences = np.array(differences)  # Convert list to np.array\n",
    "            # This is our error function, which forms a parabola\n",
    "            error = (differences ** 2).sum()\n",
    "            self.errors.append(error)\n",
    "\n",
    "            if error == 0.0:\n",
    "                print(\"Reached convergence after {} iterations\".format(idx+1))\n",
    "                break\n",
    "\n",
    "        # Return the train error at each iteration\n",
    "        return self.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig = iris.data\n",
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Shortcut, don't do this. Always do fit(train_X) and tranform on train_X and test_X\n",
    "X = scaler.fit_transform(X_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_originals = iris.target\n",
    "y_originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to change our problem from having 3 classes to having two.\n",
    "# Let's use label \"1\" for anything that is 1 or 2, and label \"-1\" for\n",
    "# anything that is currently .\n",
    "y = []\n",
    "\n",
    "for y_orig in y_originals:\n",
    "    if y_orig >= 1:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(-1)\n",
    "\n",
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached convergence after 2 iterations\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(random_state=seed)\n",
    "errors = perceptron.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "45\n",
      "Train accuracy: 100.00%\n",
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "train_acc = perceptron.score(train_X, train_y)\n",
    "test_acc = perceptron.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7795133   0.80065426 -1.3412724  -1.31297673]\n",
      "y: -1\tpred: [-1]\n"
     ]
    }
   ],
   "source": [
    "# Try for yourself to see if this worked\n",
    "sample_x = train_X[11]\n",
    "sample_y = test_y[11]\n",
    "\n",
    "pred = perceptron.predict(sample_x)\n",
    "\n",
    "print(sample_x)\n",
    "print(\"y: {}\\tpred: {}\".format(sample_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "Use the `people.csv` dataset again and use these same techniques and this perceptron to see what performance you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

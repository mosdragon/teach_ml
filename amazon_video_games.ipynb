{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Amazon Video Game Reviews\n",
    "\n",
    "We'll be applying this to a dataset of 210,000 Amazon Video Game reviews.\n",
    "\n",
    "\n",
    "\n",
    "Just as we did before, we'll use the text of the review to create our _features_, and we'll have 3 possible labels:\n",
    "- `positive` for all reviews of _4_ and _5_ stars\n",
    "- `neutral` for all reviews of _3_ stars\n",
    "- `negative` for all reviews of _2_ stars or below.\n",
    "\n",
    "The dataset will be at `datasets/video_games_amazon_cleaned.json`. This dataset was taken from [UCSD](http://jmcauley.ucsd.edu/data/amazon/) and was modified to remove fields we don't want/need.\n",
    "\n",
    "__Resources:__\n",
    "- [Scikit-learn cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "- [Wiki: Tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- [Wiki: Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# This built-in will help us out for Counting words\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# DictVectorizer will turn a list of dictionaries (BOWs in this case)\n",
    "# into a Matrix.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# To create test/train splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to stop the barrage of warning messages we'll get later\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed to use later.\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1st shipment received a book instead of the ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I got this version instead of the PS3 version,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I had Dirt 2 on Xbox 360 and it was an okay ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText\n",
       "0        1  Installing the game was a struggle (because of...\n",
       "1        4  If you like rally cars get this game you will ...\n",
       "2        1  1st shipment received a book instead of the ga...\n",
       "3        3  I got this version instead of the PS3 version,...\n",
       "4        4  I had Dirt 2 on Xbox 360 and it was an okay ga..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import your dataset as usual\n",
    "dataset = \"datasets/video_games_amazon_cleaned.json\"\n",
    "\n",
    "# NOTE: We are using read_json() here instead. Recal that JSON format\n",
    "# looks like lines of Python dictionaries.\n",
    "# The lines=True option is saying \"treat each line as it's own record\"\n",
    "df = pd.read_json(dataset, lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1st shipment received a book instead of the ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I got this version instead of the PS3 version,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I had Dirt 2 on Xbox 360 and it was an okay ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      1  Installing the game was a struggle (because of...\n",
       "1      4  If you like rally cars get this game you will ...\n",
       "2      1  1st shipment received a book instead of the ga...\n",
       "3      3  I got this version instead of the PS3 version,...\n",
       "4      4  I had Dirt 2 on Xbox 360 and it was an okay ga..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's rename the columns to \"text\" and \"stars\" like we did\n",
    "# in the previous Amazon dataset\n",
    "\n",
    "# Let's create a dict where index is \"current column name\" and the\n",
    "# value is \"desired column name\"\n",
    "mapping = {\"overall\": \"stars\", \"reviewText\": \"text\"}\n",
    "\n",
    "# Documentation can be found at\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html\n",
    "df.rename(mapper=mapping, axis=1, inplace=True)\n",
    "\n",
    "# Now let's view the first 5 again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>231780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.086397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.202330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               stars\n",
       "count  231780.000000\n",
       "mean        4.086397\n",
       "std         1.202330\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         5.000000\n",
       "75%         5.000000\n",
       "max         5.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous Amazon dataset, we're seeing the data skewed towards _4_'s and _5_'s\n",
    "\n",
    "Let's get some visualizations going to help us understand what's going on. again. Below, we're going to group our DataFrame rows by the column we care about, `stars`. \n",
    "\n",
    "__Note:__ we're going to get rid of this column soon since we care about `neutral` vs `negative` vs `positive`, not the actual number of stars someone gave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "stars        \n",
       "1       14853\n",
       "2       13663\n",
       "3       28275\n",
       "4       54804\n",
       "5      120185"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the counts of our columns after we group them by star count\n",
    "df_star_counts = df.groupby(\"stars\").count()\n",
    "df_star_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text field is just counting how many records we have with that particular number of starts. Again, in this dataset we also have far more _4_'s and _5_'s than anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1360f7550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEBCAYAAAC5R5gUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGIJJREFUeJzt3X+QXeV93/H3SiskgVaApZWBYEgZR1/UpEaxEfaEH6Y1dkbBwfW0QC3VmCaAKRjbaajbjCXbzeBMbU9gwIkSjwojZkSxZ3DrOjE0P0gTyeNATcYmHX58x25t2YCwt5JdaVl20Wq3f5xnl+vVSrv3udq9C7xfM5q553ueZ+93H93dzz3n3Hu3Z3x8HEmSaizqdgOSpFcuQ0SSVM0QkSRVM0QkSdUMEUlSNUNEklTNEJEkVTNEJEnVDBFJUjVDRJJUzRCRJFXr7XYDc2ApsAHYCxzuci+S9EqxGDgd+CYwMttJr8YQ2QDs7nYTkvQKdTHw9dkOfjWGyF6An/zkBcbG6j6heNWqFezbN3hcmzoe7Ks99tUe+2rPq62vRYt6OPXUk6D8Dp2tV2OIHAYYGxuvDpGJ+QuRfbXHvtpjX+15lfbV1mUAL6xLkqoZIpKkaoaIJKmaISJJqjarC+sRsRL4BvDuzPx+RNwAfBgYBx4DPpiZL0XEemA7cDKwC7gxM0cj4ixgJ7AGSGBzZg5GxCnAfcA5wABwVWY+HxEnAHcD5wMvApsy8+nj921Lko6HGY9EIuKtNK8ZXlu21wL/FvgV4E3la9xchu8EbsnMtUAPcH2pbwO2Zea5NKGztdRvA3Zn5jqa8Lmz1D8MvFDqHwXu7eB7lCTNkdmczrqeJiSeK9sjwL/OzAOZOQ78L+CsiDgbWJ6Zj5RxO4ArI2IJcAnwQGu93L6c5kgE4H5gYxk/Wc/MXcDqcjQjSVpAZjydlZnXAUTExPYeYE+p9QMfAq4FzuBn36SyFzgTWA0cyMzRKXVa55TTXgeA/mN8rR+0+f1J0qvS6BiMHBo9or5s6KV57aP6zYYR8XPAQ8DdmfnXEfEr0wwbozmtNV2dY+w71pxZWbVqRTvDj9Df39fR/LliX+2xr/bYV3u62deP9w/x9P/Zd0T9zScuZc089lUVIhFxLvDfgc9n5u+X8rPAaS3DTqc5BTYArIyIxZl5uKXeOueZiOgFVgL7WurfnfK1Zm3fvsHqd2329/cxMHCwau5csq/22Fd77Ks93e5raGSUg4PD0+6r6WvRop6qJ99tv8Q3IvqAPwe2tATIxGmu4Yi4sJSuAR7KzEM0H4h4dWu93H6wbFP27y7jJ+sRcREwnJmeypKkBabmSOQ64PXArRFxa6l9NTM/AWwGtpeg+RZwV9l/E3BvRGyhua7xvlLfCuyIiCeAn5b5AJ8HvlDqI8D7K/qUJM2xWYdIZv58uXlH+TfdmMeBC6ap7wEunaa+H7himvow8IHZ9iZJ6g7fsS5JqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqtY7m0ERsRL4BvDuzPx+RFwG3A4sB76UmVvKuPXAduBkYBdwY2aORsRZwE5gDZDA5swcjIhTgPuAc4AB4KrMfD4iTgDuBs4HXgQ2ZebTx+27liQdFzMeiUTEW4GvA2vL9nLgHuA9wDpgQ0RsLMN3Ardk5lqgB7i+1LcB2zLzXOAxYGup3wbszsx1NOFzZ6l/GHih1D8K3NvJNylJmhuzOZ11PXAz8FzZvgD4TmZ+LzNHaYLjyog4G1iemY+UcTtKfQlwCfBAa73cvpzmSATgfmBjGT9Zz8xdwOpyNCNJWkBmDJHMvC4zd7eUzgD2tmzvBc48Rn01cKAETmv9Z75W2X8A6D/G15IkLSCzuiYyRc80tbGKes3XmrVVq1a0M/wI/f19Hc2fK/bVHvtqj321p5t9je8fom/Fsmn3zWdfNSHyLHBay/bpNKe6jlYfAFZGxOLMPNxSb/1az0REL7AS2NdS/+6UrzVr+/YNMjY23s6USf39fQwMHKyaO5fsqz321R77ak+3+xoaGeXg4PC0+2r6WrSop+rJd81LfB8FIiLeGBGLgU3AQ5m5BxiOiAvLuGtK/RCwG7i6tV5uP1i2Kft3l/GT9Yi4CBjOzB9U9CpJmkNth0hmDgPXAl8GngSe5uWL5puBOyLiKeAk4K5Svwm4ISKeBC4GtpT6VuBtEfFEGXNzqX8eWFrqdwHvb7dPSdLcm/XprMz8+ZbbDwPnTTPmcZpXb02t7wEunaa+H7himvow8IHZ9iZJ6g7fsS5JqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKq9XYyOSL+JfA7ZfOhzLw1ItYD24GTgV3AjZk5GhFnATuBNUACmzNzMCJOAe4DzgEGgKsy8/mIOAG4GzgfeBHYlJlPd9KvJOn4qj4SiYgTgbuAtwPnARdHxGU0QXFLZq4FeoDry5RtwLbMPBd4DNha6rcBuzNzHU343FnqHwZeKPWPAvfW9ipJmhudnM5aXOafBCwp/w4ByzPzkTJmB3BlRCwBLgEeaK2X25fTHIkA3A9sLOMn65m5C1hdjmYkSQtE9emszDwYEVuBp2lON/018BKwt2XYXuBMYDVwIDNHp9QBzpiYU057HQD6W+tT5vxgNv2tWrWi/W+qRX9/X0fz54p9tce+2mNf7elmX+P7h+hbsWzaffPZV3WIRMSbgN8Azgb+H81prHdNM3SM5rTWdHWOse9Yc2a0b98gY2Pjsx3+M/r7+xgYOFg1dy7ZV3vsqz321Z5u9zU0MsrBweFp99X0tWhRT9WT705OZ/0q8HBm/jgzR2hOUV0KnNYy5nTgOZoL5isjYvGUOsCzE3MiohdYCexrrU8zR5K0AHQSIo8Dl0XESRHRA/w68DfAcERcWMZcQ/OqrUPAbuDq1nq5/WDZpuzfXcZP1iPiImA4M2d1KkuSND+qQyQz/5zmQvjfAX9Pc2H9PwKbgTsi4imai+53lSk3ATdExJPAxcCWUt8KvC0inihjbi71zwNLS/0u4P21vUqS5kZH7xPJzM8An5lSfhy4YJqxe2hOd02t7weumKY+DHygk/4kSXPLd6xLkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqr1djI5In4d+BRwEvBnmfmRiLgMuB1YDnwpM7eUseuB7cDJwC7gxswcjYizgJ3AGiCBzZk5GBGnAPcB5wADwFWZ+Xwn/UqSjq/qI5GIOAf4Y+A9wD8C3hwRG4F7Sm0dsKHUoAmKWzJzLdADXF/q24BtmXku8BiwtdRvA3Zn5jqa8LmztldJ0tzo5HTWe2mONJ7JzEPA1cAQ8J3M/F5mjtIEx5URcTawPDMfKXN3lPoS4BLggdZ6uX05zZEIwP3AxjJekrRAdHI6643ASxHxZ8BpwJ8ATwB7W8bsBc4EzjhKfTVwoAROa53WOeW01wGgH3iug54lScdRJyHSS3MUcSkwCPw3miORqcZoTl+1U2eGfTNatWrFbIdOq7+/r6P5c8W+2mNf7bGv9nSzr/H9Q/StWDbtvvnsq5MQeR74y8wcAIiIr9CcijrcMuZ0miOHZ2mOVqbWB4CVEbE4Mw+31GmZ80xE9AIrgX2zbW7fvkHGxsZrvi/6+/sYGDhYNXcu2Vd77Ks99tWebvc1NDLKwcHhaffV9LVoUU/Vk+9Oron8KfCrEXFKRCwGNtJc24iIeGOpbQIeysw9wHBEXFjmXlPqh4DdNNdTJuvl9oNlm7J/dxkvSVogqkMkMx8FPgt8HXgS2AP8EXAt8OVSe5qXL5pvBu6IiKdoXhJ8V6nfBNwQEU8CFwNbSn0r8LaIeKKMubm2V0nS3OjofSKZeQ/NS3pbPQycN83Yx4ELpqnvobmuMrW+H7iik/4kSXPLd6xLkqoZIpKkaoaIJKmaISJJqtbRhXVJmgujYzByqPkgi/H9QwyNNLeXLuml16e+C4ohImnBGTk0yjef+hEAfSuWTb6pbsO619O71F9bC4mZLkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqhkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpWsd/8T4iPgf0Z+a1EbEe2A6cDOwCbszM0Yg4C9gJrAES2JyZgxFxCnAfcA4wAFyVmc9HxAnA3cD5wIvApsx8utNeJUnHV0dHIhHxDuDaltJO4JbMXAv0ANeX+jZgW2aeCzwGbC3124DdmbmOJnzuLPUPAy+U+keBezvpU5I0N6pDJCJeB3wa+L2yfTawPDMfKUN2AFdGxBLgEuCB1nq5fTnNkQjA/cDGMn6ynpm7gNXlaEaStIB0cjrrC8DHgTeU7TOAvS379wJnAquBA5k5OqX+M3PKaa8DQP8xvtYPZtvcqlUr2vlejtDf39fR/LliX+2xr/YslL7G9w/Rt2LZ5PbE7RNPXEr/607sVltH6OZ6TV2jVvPZV1WIRMR1wA8z8+GIuLaUe6YZOnaMeu2cWdm3b5CxsfF2pkzq7+9jYOBg1dy5ZF/tsa/2LKS+hkZGOTg4DDQBMnF7aGiEgcOHu9napG6vV+saTVXT16JFPVVPvmuPRK4GTo+IbwOvA1YA48BpLWNOB56juWC+MiIWZ+bhljrAs2XOMxHRC6wE9rXUvzvla0mSFpCqayKZ+c7M/KXMXA98AvhqZv4rYDgiLizDrgEeysxDwG6a4Jmsl9sPlm3K/t1l/GQ9Ii4ChjNz1qeyJEnzo+OX+E6xGdgeEX3At4C7Sv0m4N6I2EJzXeN9pb4V2BERTwA/LfMBPg98odRHgPcf5z4lScdBxyGSmTtoXnFFZj4OXDDNmD3ApdPU9wNXTFMfBj7QaW+SpLnlO9YlSdUMEUlSNUNEklTNEJEkVTNEJEnVDBFJUjVDRJJUzRCRJFUzRCRJ1QwRSVI1Q0SSVM0QkSRVM0QkSdUMEUlSNUNEklTteP9RKkkzODj0Ei+MjB5RX7qkl16f1ukVxhCR5tmLw6N886kfHVHfsO719C71R1KvLD7vkSRVM0QkSdUMEUlSNUNEklTNEJEkVTNEJEnVDBFJUjVDRJJUraN3NkXEJ4GryubXMvNjEXEZcDuwHPhSZm4pY9cD24GTgV3AjZk5GhFnATuBNUACmzNzMCJOAe4DzgEGgKsy8/lO+pUkHV/VRyIlLN4F/DKwHnhLRLwPuAd4D7AO2BARG8uUncAtmbkW6AGuL/VtwLbMPBd4DNha6rcBuzNzHU343FnbqyRpbnRyOmsv8NuZ+VJmHgKeAtYC38nM72XmKE1wXBkRZwPLM/ORMndHqS8BLgEeaK2X25fTHIkA3A9sLOMlSQtEdYhk5hMToRARvwBcDYzRhMuEvcCZwBlHqa8GDpTAaa3TOqfsPwD01/YrSTr+Ov60t4j4ReBrwK3AISCmDBmjOX011bHqzLBvRqtWrZjt0Gn19/d1NH+u2Fd7FmJfP94/RN+KZUfUTzxxKf2vO7ELHb1soazX+JQ1mri9ENaoVTfXa+oatZrPvjq9sH4h8GXgo5n5xYh4O3Bay5DTgeeAZ49SHwBWRsTizDzcUqdlzjMR0QusBPbNtrd9+wYZGxuv+r76+/sYGDhYNXcu2Vd7FmpfLF7MwcHhI8pDQyMMHD7chYYaC2m9hkZGJ9eob8WyydvdXqNW3V6v1jWaqqavRYt6qp58d3Jh/Q3AV4BNmfnFUn602RVvjIjFwCbgoczcAwyX0AG4ptQPAbtpToVN1svtB8s2Zf/uMl6StEB0ciRyK7AMuD1i8gzWHwPX0hydLKMJgomL5puB7RHRB3wLuKvUbwLujYgtwA+A95X6VmBHRDwB/LTMlyQtINUhkpkfAT5ylN3nTTP+ceCCaep7gEunqe8HrqjtT5I093zHuiSpmiEiSapmiEiSqhkikqRqHb/Z8NXo4NBLvDAyekR96ZJeeo1dSZpkiEzjxeFRvvnUj46ob1j3enqXumSSNMHfiKoyOgYjh5qjtfH9QwyVIzeP1qTXFkNEVUYOvXy01vqxFB6tSa8tPmeUJFUzRCRJ1QwRSVI1Q0SSVM0QkSRVM0QkSdUMEUlSNUNEklTNEJEkVTNEJEnVDJFp/Oan/6LbLbyi/OEDj3e7hVcUH1/t8fHVnvl+fBkikqRqhogkqZohIkmqZohIkqoZIpKkaoaIJKmaISJJqrag/45pRGwCtgAnAHdk5h92uSVJUosFeyQSET8HfBq4CDgPuCEi/mF3u5IktVrIRyKXAX+VmfsBIuIB4J8DvzvDvMUAixb1VN/xmlOXc+KyJUfUexcv6ujrHg/dvv8JvYsXTa5R63othDVqtZB6meDja2Y+vmbWukat1py6vKqvljmL25nXMz4+3vadzYeI+B3gpMzcUravAy7IzBtmmHoRsHuu+5OkV6mLga/PdvBCPhKZLkrHZjHvmzSLsBc4fFw7kqRXr8XA6TS/Q2dtIYfIszRhMOF04LlZzBuhjRSVJE363+1OWMgh8pfApyKiH3gB+GfATKeyJEnzaMG+OisznwU+DvwP4NvAf87M/9ndriRJrRbshXVJ0sK3YI9EJEkLnyEiSapmiEiSqhkikqRqC/klvnMuIlYC3wDenZnfn7JvPbAdOBnYBdyYmaMLoK9PAL8J/KSUts/HB1NGxCeBq8rm1zLzY1P2d2W9ZtFXV9ar3Pfv0nxUzzhwd2bePmV/t9Zspr66uWafA/oz89op9bOAncAaIIHNmTk4Hz3N0Nc1wGeAH5XS1zLz4/PU018BrwcOldIHM/PRlv2XAbcDy4EvTXz6x/H2mj0SiYi30rwpce1RhuwEbsnMtTTvnr9+gfS1AfgXmbm+/JuPALkMeBfwy8B64C0R8d4pw+Z9vWbZ17yvV+nt7cA/Ad4EnA/cEhExZVg31mw2fXVrzd4BXHuU3duAbZl5LvAYsHU+eppFXxuAf9OyVvMVID3AucB5LffdGiDLgXuA9wDrgA0RsXEuennNhgjND+zNTPMu+Ig4G1iemY+U0g7gym73VZwP/LuI+PuI+IOIWDYPPe0FfjszX8rMQ8BTwFkTO7u4Xsfsq+jGepGZfwP843JksYbmqP+Fif3dWrOZ+irmfc0i4nU0n9r9e9PsWwJcAjxQSjuYp5/HY/VVbACuiYjHI2JnRJw6H30BQXMk+VC57w9N2X8B8J3M/F75v97JHK3ZazZEMvO6zDzaBzWeQfMLasJe4My57+rYfUXECuBbwK3Am4FTmIdnZJn5xMQvu4j4BeBq4MGWIV1Zr5n66tZ6tfR3KCL+A/Ak8DDNR/lM6OZj7Kh9dXHNvkDz5uKfTLNvNXCg5VTfvK3VDH1N9PIpmiPhHwJ/MD9tcSrN/90/Bd4B3BgR72zZP2+Pr9dsiMyg9sMf51RmDmbmr2Xmd8sP1O8DvzZf9x8Rvwj8BXBrZn6nZVdX1+tofXV7vUoPnwT6gTfws6erurpmR+urG2tWPqH7h5n58FGGdGWtZtEXmfnezHw0M8eBzzJPj6/M/NvMvCYzX8jM/wvcPeW+523NDJHpPQuc1rI92w9/nFMRcVZE/EZLqYeXL6rN9X1fSPPM599n5r1TdndtvY7VV5fX69xy4ZzMHAL+C811iAldWbOZ+urSml0NvCsivk3z94KuiIg7WvYPACsjYuLvXMzX4+uYfUXEyRHxWy3j5/PxdVG5VnO0+563x5chMo3M3AMMl19QANcAD3WxpQkvAp+NiH9QLqzdDPzXub7TiHgD8BVgU2Z+cer+bq3XTH3RpfUqzgG2R8TSiDiB5gLn5KdLd/Exdsy+6MKaZeY7M/OXMnM98Angq5n5Wy37D9H8jaCrS2le1mqmvoBB4GPlxTAAH2L+Hl+nAJ+LiGUR0Qd8YMp9PwpERLyxhO8m5mjNDJEWEfFgRJxfNjcDd0TEU8BJwF3d7iszB4APAn9C8zLHHprTDXPtVmAZcHtEfLv8u3EBrNcx++riepGZD9Jcn/kW8HfANzLzi91es5n66uaaTRUR/ykiriibN9H8iewnaf5ExJy8XLWdvjLzMM3Ly/+o/B++BfjYsWcfH5n5p8DXePn/8Z7M/NvyM3BGZg7TvKLsyzTXvp7m5RcmHFd+AKMkqZpHIpKkaoaIJKmaISJJqmaISJKqGSKSpGqGiCSpmiEiSapmiEiSqv1/JvX0TdRacpIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's just focus on the \"text\" column and get the values out.\n",
    "# The list we get here will be the number of examples we have with 1-star,\n",
    "# with 2-star, and so on\n",
    "counts = df_star_counts[\"text\"].values\n",
    "\n",
    "# We'll make an array with the star counts\n",
    "# So two 1's and four 2's would be: [1, 1, 2, 2, 2, 2]\n",
    "stars = []\n",
    "for idx, count in enumerate(counts):\n",
    "    star_num = idx + 1\n",
    "    stars += [star_num] * count \n",
    "\n",
    "# Here we can make a histogram of the stars. Looks like almost all the ratings are 4's or 5's\n",
    "sns.distplot(stars, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.50%\n"
     ]
    }
   ],
   "source": [
    "# Let's count the total number of examples we have\n",
    "n_samples = sum(counts)\n",
    "\n",
    "# Now let's count only the \"positive\" examples, meaning 4's and 5's\n",
    "n_positive = sum(counts[3:])\n",
    "\n",
    "# Compute the ratio of \"positive\" examples to everything else.\n",
    "positive_ratio = n_positive / n_samples\n",
    "print(\"{:.2f}%\".format(positive_ratio * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much nicer dataset than the first _amazon_ dataset we used because we would only get around 75% test accuracy if all we did was predict `positive`.\n",
    "\n",
    "## Our Task\n",
    "Again, we'll use the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) model to predict `positive`, `neutral` or `negative`.\n",
    "\n",
    "Afterwards, we'll improve our model by using [Term-frequency-Inverse-Document-Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). You will do this on your own using `TfidfTransformer`.\n",
    "\n",
    "Important things to understand about tf-idf:\n",
    "- tf-idf is a measure per-word per-document (just like the counts are per-word per-document)\n",
    "- tf-idf increases each time a word is used in a document\n",
    "- tf-idf inversely weighs that same word if the entire dataset uses that word frequently\n",
    "- __Intuition__: I really care about a word `w` if `w` is used frequently in my document but not frequently in my dataset.\n",
    "\n",
    "_Document_ here means each example of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess our data. Let's build our counts of words.\n",
    "# TODO: Implement this function, you can copy it from previous assignment.\n",
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    TODO: \n",
    "    1. Lowercase everything in the text\n",
    "    2. Replace all punctuation except exclamation point & apostrophe with space\n",
    "    HINT: use text.replace() function. It's called \"string replacement\"\n",
    "    3. Replace all apostrophes with empty strings\n",
    "    4. Replace all \"!\" with  \" ! \". This will treat each \"!\" as a word which will\n",
    "    help with our BOW model\n",
    "    6. Split up the text into words, and make a list called \"words\" that holds all the words\n",
    "    including \"!\"\n",
    "    7. Finally, return Counter(words), which will be a dictionary with counts of the occurence\n",
    "    of each word\n",
    "    \"\"\"\n",
    "    punctuation = [\".\", \",\", \"(\", \")\", \":\", \";\", '\"', \"\\n\", \"\\\\\", \"\\/\", \"&\", \"+\", \"-\"] # HINT: filter these out\n",
    "    words = text.split(\" \")\n",
    "    return Counter(keep_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 1.63 s, total: 19.5 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_vals = df[\"text\"].values\n",
    "\n",
    "# Apply the bag_of_words to the functions and store those in a list called bows.\n",
    "bows = []\n",
    "for text in text_vals:\n",
    "    bow = bag_of_words(text)\n",
    "    bows.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we have the bag of words for ALL text in our dataset. Let's count up occurrences\n",
    "# for each word in the dataset. Later, we'll filter out the infrequence words we don't want.\n",
    "word_occurrences = Counter()\n",
    "\n",
    "for bow in bows:\n",
    "    word_occurrences.update(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector y, which has our labels\n",
    "y_stars = df[\"stars\"].values\n",
    "y = []\n",
    "\n",
    "for stars in y_stars:\n",
    "    if stars >= 4:\n",
    "        y.append(\"positive\")\n",
    "    elif stars == 3:\n",
    "        y.append(\"neutral\")\n",
    "    else:\n",
    "        y.append(\"negative\")\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a helper function that accepts min_occurence and stop_words\n",
    "# and produces a new vocabulary\n",
    "def create_vocab(min_occurrence, stop_words):\n",
    "    # Now, let's filter out the words that have < 30 occurences\n",
    "    vocab = set()\n",
    "    removed_vocab = set()\n",
    "\n",
    "    for word, count in word_occurrences.items():\n",
    "        if count >= min_occurrence and word not in stop_words:\n",
    "            vocab.add(word)\n",
    "        else:\n",
    "            removed_vocab.add(word)\n",
    "            \n",
    "    return vocab, removed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create a bag_of_words_using_vocab function that does the same thing as\n",
    "# bag_of_words above except we remove any words that are not in the vocab set\n",
    "def bag_of_words_using_vocab(text, vocabulary=set()):\n",
    "    bow = bag_of_words(text)\n",
    "    words = list(bow.keys())\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            bow.pop(word)\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll create a helper function to take care of making new data splits\n",
    "def use_new_vocab(vocabulary):\n",
    "    new_bows = []\n",
    "\n",
    "    for text in text_vals:\n",
    "        # Use bag_of_words_using_vocab with the vocabulary\n",
    "        new_bow = bag_of_words_using_vocab(text, vocabulary=vocabulary)\n",
    "        new_bows.append(new_bow)\n",
    "    \n",
    "    # Now, creater vectors from these dictionaries\n",
    "    X = DictVectorizer().fit_transform(new_bows)\n",
    "    \n",
    "    # Create test/train splits\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=seed)\n",
    "    return (train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Train Estimators\n",
    "\n",
    "Use `create_vocab()` and `use_new_vocab` to create `train_X, test_X, train_y, test_y`, which are the training and test sets generated from the updated vocabulary words. Do __at least__ 3 trials here.\n",
    "\n",
    "Apply these 2 algorithms:\n",
    "- [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "__NOTE:__ Use cross-validation only for models that don't take a long time to train. If you're training models for > 5 mins, tweak things around so you aren't doing that. Remember, we're trying to beat 75% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17819\n",
      "261013\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Trial 1\n",
    "min_occurence = 1  # TODO: Change this to a big number\n",
    "stop_words = []  # TODO: Change this to have some stop-words\n",
    "\n",
    "vocab, removed_vocab = create_vocab(min_occurence, stop_words)\n",
    "print(len(vocab))\n",
    "print(len(removed_vocab))\n",
    "\n",
    "# Create test/train splits\n",
    "train_X, test_X, train_y, test_y = use_new_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 76.97%\n",
      "Test accuracy: 75.53%\n",
      "CPU times: user 449 ms, sys: 27.6 ms, total: 476 ms\n",
      "Wall time: 478 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "mb_train_acc = mnb.score(train_X, train_y)\n",
    "mb_test_acc = mnb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MB Train accuracy: {:.2f}%\".format(mb_train_acc * 100))\n",
    "print(\"MB Test accuracy: {:.2f}%\".format(mb_train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.06%\n",
      "Test accuracy: 76.72%\n",
      "CPU times: user 2min 23s, sys: 1.01 s, total: 2min 24s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "rf_train_acc = rf.score(train_X, train_y)\n",
    "rf_test_acc = rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF Train accuracy: {:.2f}%\".format(rf_train_acc * 100))\n",
    "print(\"RF Test accuracy: {:.2f}%\".format(rf_test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8680\n",
      "270152\n",
      "CPU times: user 50.7 s, sys: 9.35 s, total: 1min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Trial 2\n",
    "min_occurence = 1  # TODO: Change this to a big number\n",
    "stop_words = []  # TODO: Change this to have some stop-words\n",
    "\n",
    "vocab, removed_vocab = create_vocab(min_occurence, stop_words)\n",
    "print(len(vocab))\n",
    "print(len(removed_vocab))\n",
    "\n",
    "# Create test/train splits\n",
    "train_X, test_X, train_y, test_y = use_new_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 442 ms, sys: 40.5 ms, total: 483 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "mb_train_acc = mnb.score(train_X, train_y)\n",
    "mb_test_acc = mnb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB Train accuracy: 76.15%\n",
      "MB Test accuracy: 76.15%\n"
     ]
    }
   ],
   "source": [
    "print(\"MB Train accuracy: {:.2f}%\".format(mb_train_acc * 100))\n",
    "print(\"MB Test accuracy: {:.2f}%\".format(mb_train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 1.11 s, total: 2min 38s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "rf_train_acc = rf.score(train_X, train_y)\n",
    "rf_test_acc = rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Train accuracy: 99.08%\n",
      "RF Test accuracy: 76.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"RF Train accuracy: {:.2f}%\".format(rf_train_acc * 100))\n",
    "print(\"RF Test accuracy: {:.2f}%\".format(rf_test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5124\n",
      "273708\n",
      "CPU times: user 47.1 s, sys: 6.91 s, total: 54.1 s\n",
      "Wall time: 55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Trial 3\n",
    "min_occurence = 1  # TODO: Change this to a big number\n",
    "stop_words = []  # TODO: Change this to have some stop-words\n",
    "\n",
    "vocab, removed_vocab = create_vocab(min_occurence, stop_words)\n",
    "print(len(vocab))\n",
    "print(len(removed_vocab))\n",
    "\n",
    "# Create test/train splits\n",
    "train_X, test_X, train_y, test_y = use_new_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 416 ms, sys: 27 ms, total: 443 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "mb_train_acc = mnb.score(train_X, train_y)\n",
    "mb_test_acc = mnb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB Train accuracy: 75.71%\n",
      "MB Test accuracy: 75.71%\n"
     ]
    }
   ],
   "source": [
    "print(\"MB Train accuracy: {:.2f}%\".format(mb_train_acc * 100))\n",
    "print(\"MB Test accuracy: {:.2f}%\".format(mb_train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 1.02 s, total: 2min 45s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "rf_train_acc = rf.score(train_X, train_y)\n",
    "rf_test_acc = rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Train accuracy: 99.12%\n",
      "RF Test accuracy: 76.74%\n"
     ]
    }
   ],
   "source": [
    "print(\"RF Train accuracy: {:.2f}%\".format(rf_train_acc * 100))\n",
    "print(\"RF Test accuracy: {:.2f}%\".format(rf_test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Evaluation\n",
    "\n",
    "Evaluate how well we did using Confusion matrices and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Improve the Model\n",
    "\n",
    "Try these approaches to improve your model:\n",
    "- Try changing the minimum occurences\n",
    "- Try different stop-words\n",
    "- Use CV for the faster models\n",
    "- Try [SGDClassifier](http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use) and make sure to use `StandardScaler` here just like this document shows.\n",
    "\n",
    "__GOAL:__ See how high you can get your accuracy. I managed to get _76.7_% accuracy with very little tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Use Tf-Idf\n",
    "Use the [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) on the output from the `DictVectorizer`. This will change our values from being `counts` of a word in a document to being the `tf-idf` value for that word in that document.\n",
    "\n",
    "__GOAL:__ Get 80-85% accuracy. Students with the top 3 accuracies will get a prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

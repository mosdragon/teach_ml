{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Amazon Reviews\n",
    "\n",
    "Now, we're going to take a deep dive into Natural Language Processing. One of the most common tasks in this domain is _sentiment analysis_, detecting whether the author had a _positive_ or _negative_ tone. We'll be applying this to a dataset of 34,000 Amazon product reviews.\n",
    "\n",
    "We'll use the text of the reviews and titles as our _features_, and we'll have 3 possible labels:\n",
    "- `positive` for all reviews of _4_ and _5_ stars\n",
    "- `neutral` for all reviews of _3_ stars\n",
    "- `negative` for all reviews of _2_ stars or below.\n",
    "\n",
    "The dataset will be at `datasets/amazon_reviews.csv`. They have been adapted from a larger dataset available from [Kaggle](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products/version/3#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# To create test/train splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to stop the barrage of warning messages we'll get later\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed to use later.\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numHelpful</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>Kindle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>very fast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>Beginner tablet for our 9 year old son.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>Good!!!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>Fantastic Tablet for kids</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numHelpful                                               text  \\\n",
       "0         0.0  This product so far has not disappointed. My c...   \n",
       "1         0.0  great for beginner or experienced person. Boug...   \n",
       "2         0.0  Inexpensive tablet for him to use and learn on...   \n",
       "3         0.0  I've had my Fire HD 8 two weeks now and I love...   \n",
       "4         0.0  I bought this for my grand daughter when she c...   \n",
       "\n",
       "                                     title  stars  \n",
       "0                                   Kindle      5  \n",
       "1                                very fast      5  \n",
       "2  Beginner tablet for our 9 year old son.      5  \n",
       "3                                  Good!!!      4  \n",
       "4                Fantastic Tablet for kids      5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import your dataset as usual\n",
    "dataset = \"datasets/amazon_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numHelpful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34129.000000</td>\n",
       "      <td>34622.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.630226</td>\n",
       "      <td>4.584513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.216162</td>\n",
       "      <td>0.735689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>814.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         numHelpful         stars\n",
       "count  34129.000000  34622.000000\n",
       "mean       0.630226      4.584513\n",
       "std       13.216162      0.735689\n",
       "min        0.000000      1.000000\n",
       "25%        0.000000      4.000000\n",
       "50%        0.000000      5.000000\n",
       "75%        0.000000      5.000000\n",
       "max      814.000000      5.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results from the table above. Does something seem off to you? If you know what [quartiles](https://www.mathsisfun.com/data/quartiles.html) are, then you should be seeing a pattern in the data from the table above.\n",
    "\n",
    "Let's get some visualizations going to help us understand what's going on. Below, we're going to group our DataFrame rows by the column we care about, `stars`. __Note:__ we're going to get rid of this column soon since we care about `neutral` vs `negative` vs `positive`, not the actual number of stars someone gave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numHelpful</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1475</td>\n",
       "      <td>1499</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8480</td>\n",
       "      <td>8541</td>\n",
       "      <td>8541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23411</td>\n",
       "      <td>23770</td>\n",
       "      <td>23769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       numHelpful   text  title\n",
       "stars                          \n",
       "1             375    410    410\n",
       "2             388    402    402\n",
       "3            1475   1499   1499\n",
       "4            8480   8541   8541\n",
       "5           23411  23770  23769"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the counts of our columns after we group them by star count\n",
    "df_star_counts = df.groupby(\"stars\").count()\n",
    "df_star_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text and title fields are effectively just counting the rows for us now. Ignoring the discrepancy in counts for text and title (this dataset is not perfect) we can see that the reviews for the products in this dataset are overwhelmingly 5's.\n",
    "\n",
    "So, this dataset is not the best because it is biased towards 4's and 5's off the bat. Consequently, we should expect that after training, any of our estimators will spit out `positive` as the prediction almost all the time. Let's crunch some numbers below to see if we could come up with a better `classifier` giving the same prediction _100%_ of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d48c668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's just focus on the \"text\" column and get the values out.\n",
    "# The list we get here will be the number of examples we have with 1-star,\n",
    "# with 2-star, and so on\n",
    "counts = df_star_counts[\"text\"].values\n",
    "\n",
    "# We'll make an array with the star counts\n",
    "# So two 1's and four 2's would be: [1, 1, 2, 2, 2, 2]\n",
    "stars = []\n",
    "for idx, count in enumerate(counts):\n",
    "    star_num = idx + 1\n",
    "    stars += [star_num] * count \n",
    "\n",
    "# Here we can make a histogram of the stars. Looks like almost all the ratings are 4's or 5's\n",
    "sns.distplot(stars, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332505343423257\n"
     ]
    }
   ],
   "source": [
    "# Let's count the total number of examples we have\n",
    "n_samples = sum(counts)\n",
    "\n",
    "# Now let's count only the \"positive\" examples, meaning 4's and 5's\n",
    "n_positive = sum(counts[3:])\n",
    "\n",
    "# TODO: Now, compute the ratio of \"positive\" examples to all examples\n",
    "positive_ratio = n_positive / n_samples\n",
    "print(positive_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So check out the number above. It sounds absurd, but we could have a _classifier_ made that does nothing but give _positive_ for every example and we'd be hitting over 90% accuracy.\n",
    "\n",
    "Ambitious though it may be, let's see if we can actually train a true classifier to improve our accuracy above what we got for _positive_ratio_.\n",
    "\n",
    "Let's start by using a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  This product so far has not disappointed. My c...      5\n",
       "1  great for beginner or experienced person. Boug...      5\n",
       "2  Inexpensive tablet for him to use and learn on...      5\n",
       "3  I've had my Fire HD 8 two weeks now and I love...      4\n",
       "4  I bought this for my grand daughter when she c...      5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: We need to preprocess our data. For now, we'll only be using the text and stars fields\n",
    "df = df[[\"text\", \"stars\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll build our counts of words.\n",
    "\n",
    "# This built-in will help us out\n",
    "from collections import Counter\n",
    "\n",
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    TODO: \n",
    "    1. Lowercase everything in the text\n",
    "    2. Replace all punctuation except exclamation point & apostrophe with space\n",
    "    HINT: use text.replace() function. It's called \"string replacement\"\n",
    "    3. Replace all apostrophes with empty strings\n",
    "    4. Replace all \"!\" with  \" ! \". This will treat each \"!\" as a word which will\n",
    "    help with our BOW model\n",
    "    6. Split up the text into words, and make a list called \"words\" that holds all the words\n",
    "    including \"!\"\n",
    "    7. Finally, return Counter(words), which will be a dictionary with counts of the occurence\n",
    "    of each word\n",
    "    \"\"\"\n",
    "    punctuation = [\".\", \",\", \"(\", \")\", \":\", \";\", '\"', \"\\\\\", \"\\/\", \"&\", \"+\", \"-\"] # HINT: filter these out\n",
    "    words = []\n",
    "    return Counter(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, pluck out the dataframe's \"text\" column values\n",
    "text_vals = df[\"text\"].values\n",
    "text_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the bag_of_words to the functions and store those in a list called bows.\n",
    "bows = []\n",
    "for text in text_vals:\n",
    "    bow = bag_of_words(text)\n",
    "    bows.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the first example rating\n",
    "df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'this': 1,\n",
       "         'product': 1,\n",
       "         'so': 1,\n",
       "         'far': 1,\n",
       "         'has': 1,\n",
       "         'not': 1,\n",
       "         'disappointed': 1,\n",
       "         '': 2,\n",
       "         'my': 1,\n",
       "         'children': 1,\n",
       "         'love': 1,\n",
       "         'to': 2,\n",
       "         'use': 1,\n",
       "         'it': 1,\n",
       "         'and': 1,\n",
       "         'i': 1,\n",
       "         'like': 1,\n",
       "         'the': 1,\n",
       "         'ability': 1,\n",
       "         'monitor': 1,\n",
       "         'control': 1,\n",
       "         'what': 1,\n",
       "         'content': 1,\n",
       "         'they': 1,\n",
       "         'see': 1,\n",
       "         'with': 1,\n",
       "         'ease': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the first example text\n",
    "print(text_vals[0])\n",
    "# Display the word counts for this text\n",
    "bows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we have the bag of words for ALL text in our dataset. Let's build a vocabulary out of this\n",
    "# Each bow is a Counter object, so we want to keep the total occurences of words in the dataset\n",
    "word_occurences = Counter()\n",
    "\n",
    "for bow in bows:\n",
    "    word_occurences.update(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's filter out the words that have at least 10 occurences\n",
    "vocab = set()\n",
    "removed_vocab = set()\n",
    "\n",
    "for word, count in word_occurences.items():\n",
    "    if  count >= 10:\n",
    "        vocab.add(word)\n",
    "    else:\n",
    "        removed_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'korea',\n",
       " 'alduts',\n",
       " 'photography',\n",
       " 'animations',\n",
       " 'reconnecting',\n",
       " 'figuratively',\n",
       " 'unimpressed',\n",
       " 'fiancée',\n",
       " 'wel',\n",
       " 'dummy',\n",
       " 'greenish',\n",
       " 'af',\n",
       " 'aphobic',\n",
       " 'amazon?',\n",
       " 'tryout',\n",
       " 'chipper',\n",
       " 'travelers',\n",
       " 'colorado',\n",
       " 'timer?',\n",
       " 'contrasts',\n",
       " 'york',\n",
       " 'guides',\n",
       " 'uodates',\n",
       " 'longest',\n",
       " 'discomfort',\n",
       " 'shareware',\n",
       " 'misunderstands',\n",
       " 'slider',\n",
       " 'dismay',\n",
       " 'browsers',\n",
       " 'skynet',\n",
       " 'kindle/tablet',\n",
       " '$55',\n",
       " 'excellently',\n",
       " 'rm',\n",
       " 'thirties',\n",
       " '$69',\n",
       " 'iam',\n",
       " 'tha',\n",
       " 'thoughtful',\n",
       " 'condense',\n",
       " 'decently',\n",
       " 'seeking',\n",
       " 'trashed',\n",
       " 'restored',\n",
       " 'thoughts',\n",
       " 'delve',\n",
       " 'centered',\n",
       " 'empty',\n",
       " '8yr',\n",
       " 'blame',\n",
       " 'caution',\n",
       " 'negotiate',\n",
       " 'september',\n",
       " 'logically',\n",
       " 'loosens',\n",
       " 'purse?',\n",
       " 'underpowered',\n",
       " 'grace',\n",
       " 'aswell',\n",
       " 'bull',\n",
       " 'boring',\n",
       " '$$$$$',\n",
       " 'tuff',\n",
       " 'repeating',\n",
       " 'supportnegative',\n",
       " 'makeup',\n",
       " 'disrupt',\n",
       " 'silence',\n",
       " 'traditionalist',\n",
       " 'command?',\n",
       " 'prodcut',\n",
       " 'paste',\n",
       " 'ffeature',\n",
       " 'moreover',\n",
       " 'soundbar/extra',\n",
       " 'agnostic',\n",
       " 'lisp',\n",
       " 'turkey',\n",
       " 'finicky',\n",
       " 'productive',\n",
       " 'saavier',\n",
       " 'possessing',\n",
       " 'discourage',\n",
       " 'links/ads',\n",
       " 'misunderstood',\n",
       " 'needy',\n",
       " 'differently',\n",
       " 'subtle',\n",
       " 'daddy',\n",
       " 'watering',\n",
       " 'granma',\n",
       " 'scenes',\n",
       " 'millennium',\n",
       " 'ford',\n",
       " 'betterand',\n",
       " 'fuller',\n",
       " 'resd',\n",
       " 'psuh',\n",
       " 'rewarding',\n",
       " 'procrastinated',\n",
       " 'music/info',\n",
       " '30am',\n",
       " 'shuffling',\n",
       " 'singer/',\n",
       " '$2',\n",
       " 'sucker',\n",
       " 'chrismas',\n",
       " 'tractors',\n",
       " 'useit',\n",
       " 'heavier/bigger',\n",
       " '191',\n",
       " 'loloaded',\n",
       " 'althought',\n",
       " 'forsook',\n",
       " 'curled',\n",
       " 'rattles',\n",
       " 'renew',\n",
       " 'saavy',\n",
       " 'com/dp/b01j2g4vbg/refcmcrrypprdttlsol4',\n",
       " 'coded',\n",
       " 'congestion',\n",
       " 'plum',\n",
       " 'steady',\n",
       " 'wind',\n",
       " 'photos/',\n",
       " 'succeeded',\n",
       " 'granddaugther',\n",
       " 'fed',\n",
       " 'modified',\n",
       " 'of/off',\n",
       " '65mbps',\n",
       " '1yr',\n",
       " 'wad',\n",
       " 'transported',\n",
       " 'neck',\n",
       " 'enthusiasm',\n",
       " 'foward/backward',\n",
       " 'document6',\n",
       " 'emailing',\n",
       " 'l8ike',\n",
       " 'unresponsive',\n",
       " 'flyer',\n",
       " 'together?',\n",
       " 'kim',\n",
       " 'awe',\n",
       " 'tv?',\n",
       " '1/10t',\n",
       " 'death',\n",
       " 'repeats',\n",
       " 'product***********************************************',\n",
       " '110',\n",
       " 'singular',\n",
       " 'visa',\n",
       " 'poetically',\n",
       " 'correction',\n",
       " 'pricey?',\n",
       " 'inaudible',\n",
       " 'wtf???',\n",
       " 'assessible',\n",
       " 'representatives',\n",
       " 'buffered',\n",
       " 'kardon',\n",
       " 'prongs',\n",
       " 'phonics',\n",
       " 'comical',\n",
       " 'replaceable',\n",
       " 'spilled',\n",
       " 'knitting',\n",
       " 'allowable',\n",
       " 'tveasy',\n",
       " 'anniversaries/birthdays',\n",
       " 'composer',\n",
       " 'twenty',\n",
       " 'end/budget',\n",
       " 'st',\n",
       " 'prome',\n",
       " 'roof',\n",
       " '80$',\n",
       " 'stepping',\n",
       " 'cringing',\n",
       " '$375',\n",
       " 'ireader',\n",
       " 'tucking',\n",
       " 'snacks',\n",
       " 'tumblr',\n",
       " 'dots/echos',\n",
       " 'aptitude',\n",
       " 'universe',\n",
       " 'voyages',\n",
       " 'doubled',\n",
       " 'deprogrammed',\n",
       " 'fear',\n",
       " 'han',\n",
       " 'warps',\n",
       " 'dry',\n",
       " 'bluray',\n",
       " 'transition',\n",
       " 'sneakily',\n",
       " 'lead',\n",
       " 'strap',\n",
       " 'defintly',\n",
       " 'ipad?',\n",
       " 'disraction',\n",
       " 'modtly',\n",
       " 'sections',\n",
       " 'intermittently',\n",
       " 'paws',\n",
       " 'thestore',\n",
       " 'lear4n',\n",
       " 'daybag',\n",
       " 'easytocarry',\n",
       " 'immersed',\n",
       " 'preordered',\n",
       " 'co_worker',\n",
       " 'pocketbook',\n",
       " 'borders',\n",
       " 'itnow',\n",
       " 'vivint',\n",
       " 'today?',\n",
       " 'taker',\n",
       " 'phones/tablets',\n",
       " 'buisness',\n",
       " 'looooooong',\n",
       " 'removing',\n",
       " 'blocked',\n",
       " 'smallish',\n",
       " 'applicable',\n",
       " 'personable',\n",
       " 'youth',\n",
       " 'meim',\n",
       " 'managedto',\n",
       " 'mlbtv',\n",
       " 'debice',\n",
       " 'slotcons',\n",
       " 'routines',\n",
       " 'planty',\n",
       " 'cery',\n",
       " 'existence',\n",
       " 'fiddled',\n",
       " 'chicken',\n",
       " 'usking',\n",
       " 'champions',\n",
       " 'wildest',\n",
       " 'beef',\n",
       " 'matching',\n",
       " 'highs',\n",
       " 'internals',\n",
       " 'priority',\n",
       " 'nocomment',\n",
       " 'iceberg',\n",
       " 'downright',\n",
       " 'insists',\n",
       " 'effitient',\n",
       " 'manages',\n",
       " '0ghz',\n",
       " 'tinkerer',\n",
       " 'bougth',\n",
       " 'references',\n",
       " 'march',\n",
       " 'weary',\n",
       " 'locals',\n",
       " 'movable',\n",
       " 'supportcons',\n",
       " 'pirce',\n",
       " '2x',\n",
       " 'grandfathers',\n",
       " 'freeing',\n",
       " 'convienece',\n",
       " 'underpriced',\n",
       " 'googled',\n",
       " 'only$35',\n",
       " 'sevices',\n",
       " 'oonz',\n",
       " 'accessble',\n",
       " 'jumps',\n",
       " 'redo',\n",
       " 'shabby',\n",
       " 'for$60',\n",
       " 'erased',\n",
       " 'clean/simple/no',\n",
       " 'sunset',\n",
       " 'headsets',\n",
       " 'critique',\n",
       " 'do?',\n",
       " 'haveit',\n",
       " 'useinterfaces',\n",
       " 'anime',\n",
       " 'ruger',\n",
       " 'notepad',\n",
       " 'loudest',\n",
       " 'millions',\n",
       " 'fore',\n",
       " 'ni',\n",
       " 'tangerine',\n",
       " 'respects',\n",
       " 'stumped',\n",
       " 'setuphttps',\n",
       " 'paided',\n",
       " 'upgrade/replacement',\n",
       " 'creates',\n",
       " 'bby01',\n",
       " 'do/tell',\n",
       " 'outrages',\n",
       " 'fmily',\n",
       " 'graders',\n",
       " 'vacuums',\n",
       " 'bien',\n",
       " 'ican',\n",
       " 'jissela',\n",
       " 'turbomode',\n",
       " 'domestic',\n",
       " 'posible',\n",
       " 'b00oqvzdjm',\n",
       " 'macos',\n",
       " 'dressed',\n",
       " 'safari',\n",
       " 'fantastics',\n",
       " 'asus',\n",
       " 'touchable',\n",
       " 'interruption',\n",
       " 'quilty',\n",
       " 'feather',\n",
       " 'everyones',\n",
       " 'nowrealize',\n",
       " 'techny',\n",
       " 'barrow',\n",
       " 'writer',\n",
       " 'regrettable',\n",
       " 'pot',\n",
       " 'aguilera',\n",
       " 'hd/4k',\n",
       " 'downs',\n",
       " 'majorly',\n",
       " 'feedbackthe',\n",
       " 'ds',\n",
       " 'estimated',\n",
       " 'restore',\n",
       " 'tabletthe',\n",
       " 'bland',\n",
       " '20%',\n",
       " 'vasr',\n",
       " 'softens',\n",
       " 'rethinking',\n",
       " 'failing',\n",
       " '5min',\n",
       " 'promo',\n",
       " 'becasue',\n",
       " 'prettymuch',\n",
       " 'carefree',\n",
       " 'visuals',\n",
       " 'withdraw',\n",
       " 'political',\n",
       " 'domain',\n",
       " 'interacts',\n",
       " 'moisture',\n",
       " 'article',\n",
       " 'skyping',\n",
       " 'wins',\n",
       " 'zelda',\n",
       " 'thos',\n",
       " 'apprehensive',\n",
       " 'secs',\n",
       " 'commutes',\n",
       " 'charts',\n",
       " 'simplify',\n",
       " 'weeks，i',\n",
       " 'wary',\n",
       " 'cruises',\n",
       " 'lacrosse',\n",
       " 'calculator',\n",
       " 'wellgreat',\n",
       " 'disclosure',\n",
       " 'eggs',\n",
       " 'alllow',\n",
       " 'mathematics',\n",
       " 'orice',\n",
       " 'sweater',\n",
       " '—\\xa0opening',\n",
       " 'sandwich',\n",
       " 'cold',\n",
       " 'children’s',\n",
       " 'unanswered',\n",
       " 'sequin',\n",
       " 'v1',\n",
       " 'jeff',\n",
       " 'emulators',\n",
       " 'elementary',\n",
       " 'unsnaps',\n",
       " 'en',\n",
       " 'todo',\n",
       " 'cach',\n",
       " '44',\n",
       " 'bluetooths',\n",
       " 'wastes',\n",
       " 'screenis',\n",
       " 'bejeweled',\n",
       " 'blueshade',\n",
       " 'llight',\n",
       " 'funnier',\n",
       " '90s',\n",
       " 'nite',\n",
       " 'network/w',\n",
       " 'samples',\n",
       " 'philosophy',\n",
       " 'shimmering',\n",
       " 'rationalize',\n",
       " 'sopranos',\n",
       " 'mixed',\n",
       " 'surgeries',\n",
       " 'alessa',\n",
       " 'colorful',\n",
       " 'burn',\n",
       " '33',\n",
       " 'kardashian',\n",
       " 'triggers',\n",
       " 'toattach',\n",
       " 'breakcons',\n",
       " '4more',\n",
       " 'lake',\n",
       " 'fort',\n",
       " '$50?',\n",
       " 'pokémon',\n",
       " 'presume',\n",
       " 'supportive',\n",
       " 'gratifying',\n",
       " 'rectify',\n",
       " 'inferior',\n",
       " 'sunscreen',\n",
       " 'supper',\n",
       " 'authorized',\n",
       " 'unskilled',\n",
       " 'promotions',\n",
       " 'uss',\n",
       " 'checks/authentication',\n",
       " 'balky',\n",
       " 'fussy',\n",
       " 'modifiable',\n",
       " 'youe',\n",
       " 'alaxa',\n",
       " 'preview',\n",
       " 'feachers',\n",
       " 'assure',\n",
       " 'deletion',\n",
       " 'firegreat',\n",
       " 'kindergartener',\n",
       " 'behemoth',\n",
       " 'thatn',\n",
       " 'cozy',\n",
       " 'synchronize',\n",
       " 'techknowledgy',\n",
       " 'surfacing',\n",
       " 'conflicting',\n",
       " 'british',\n",
       " 'spacey',\n",
       " 'xfat',\n",
       " 'amazon/',\n",
       " 'betwee',\n",
       " 'pulls',\n",
       " 'generous',\n",
       " 'pile',\n",
       " 'unbelievible',\n",
       " 'staples',\n",
       " 'tablet/reader',\n",
       " 'more??',\n",
       " 'sixth',\n",
       " 'googlke',\n",
       " 'restrictions/blocks',\n",
       " 'netlfix',\n",
       " 'echoim',\n",
       " 'arbitrarily',\n",
       " 'fareasy',\n",
       " 'litle',\n",
       " 'begiiner',\n",
       " 'shower',\n",
       " 'cramped',\n",
       " 'defined',\n",
       " 'mgr',\n",
       " 'aprice',\n",
       " 'easyto',\n",
       " 'aethetic',\n",
       " 'positions',\n",
       " 'didn���t',\n",
       " 'tweeks',\n",
       " 'europe',\n",
       " '$500',\n",
       " 'unwilling',\n",
       " 'suggestion',\n",
       " 'girly',\n",
       " 'shady',\n",
       " 'faulty',\n",
       " 'giftcards',\n",
       " 'noted',\n",
       " 'audibles',\n",
       " 'fite',\n",
       " 'expections',\n",
       " 'firetvstick',\n",
       " 'sizzled',\n",
       " 'tape',\n",
       " 'noting',\n",
       " 'competing',\n",
       " 'fiction',\n",
       " 'bypassed',\n",
       " 'lefties',\n",
       " 'unannounced',\n",
       " 'forgetting',\n",
       " 'graduates',\n",
       " 'wolf',\n",
       " 'graphicsawesome',\n",
       " 'twc/spectrum',\n",
       " 'discussed',\n",
       " 'listin',\n",
       " '33%',\n",
       " 'doodling',\n",
       " 'industry',\n",
       " 'booted',\n",
       " 'store*',\n",
       " 'monday',\n",
       " 'controllable',\n",
       " 'south',\n",
       " 'bees',\n",
       " 'connection*',\n",
       " 'sheuses',\n",
       " 'begins',\n",
       " 'protecter',\n",
       " 'acrobat',\n",
       " 'freaking',\n",
       " 'jones',\n",
       " 'backwards',\n",
       " '$199',\n",
       " 'carpal',\n",
       " 'elected',\n",
       " 'difference?',\n",
       " 'outdone',\n",
       " 'ads2',\n",
       " 'smashed',\n",
       " 'reaction',\n",
       " 'regulate',\n",
       " 'cams',\n",
       " 'gripping',\n",
       " 'nap',\n",
       " 'graciously',\n",
       " 'contacts',\n",
       " 'hefty',\n",
       " 'folds',\n",
       " 'fatigue',\n",
       " 'teplaced',\n",
       " 'liquid',\n",
       " 'overpaying',\n",
       " 'thunderstorm',\n",
       " 'procesor',\n",
       " 'groups',\n",
       " 'sunday',\n",
       " '4ktv',\n",
       " 'virtual',\n",
       " 'draining',\n",
       " 'realm',\n",
       " 'baught',\n",
       " 'money?',\n",
       " 'refuse',\n",
       " 'positioned',\n",
       " 'boasts',\n",
       " 'boils',\n",
       " '$190',\n",
       " 'active/continue',\n",
       " 'deviceq',\n",
       " 'seating',\n",
       " 'taplet',\n",
       " 'annoyances',\n",
       " 'readno',\n",
       " 'shuttle',\n",
       " 'abruptly',\n",
       " 'accessibility',\n",
       " 'washing',\n",
       " 'intertainment',\n",
       " 'forgiving',\n",
       " 'multiform',\n",
       " 'casts',\n",
       " 'dizzying',\n",
       " 'andy',\n",
       " 'fundamental',\n",
       " 'gatherings',\n",
       " 'memory?',\n",
       " '$3/month',\n",
       " 'squirly',\n",
       " 'kf',\n",
       " '120v',\n",
       " 'inevitably',\n",
       " 'controlsremote',\n",
       " 'contemplated',\n",
       " 'asphalt',\n",
       " 'mistakes',\n",
       " 'meeting',\n",
       " 'wihite',\n",
       " 'stimulation',\n",
       " 'fascinated',\n",
       " 'haiku',\n",
       " 'timers/alarms',\n",
       " 'risk',\n",
       " 'phobe',\n",
       " 'pe',\n",
       " 'describe',\n",
       " 'limiter',\n",
       " 'qualitycons',\n",
       " 'starters',\n",
       " 'consequence',\n",
       " 'hy',\n",
       " 'flick',\n",
       " 'ocasional',\n",
       " 'fruit',\n",
       " 'expire',\n",
       " 'aweosme',\n",
       " 'usefule',\n",
       " '2011',\n",
       " 'megapixel',\n",
       " 'patch',\n",
       " 'actly',\n",
       " 'unfold',\n",
       " 'exc',\n",
       " 'adopt',\n",
       " 'museum',\n",
       " 'til',\n",
       " 'sinking',\n",
       " 'satisfly',\n",
       " 'concentrated',\n",
       " 'fountains',\n",
       " 'aficionado',\n",
       " 'gigantic',\n",
       " '$88',\n",
       " 'conected',\n",
       " 'browses',\n",
       " 'thelarge',\n",
       " 'locally',\n",
       " 'launch',\n",
       " 'stretches',\n",
       " 'enjoiy',\n",
       " 'posts',\n",
       " 'insisted',\n",
       " 'chooses',\n",
       " 'meander',\n",
       " 'workarounds',\n",
       " 'compromises',\n",
       " 'brain',\n",
       " 'integral',\n",
       " 'ther',\n",
       " 'jr',\n",
       " 'nickel',\n",
       " 'coll',\n",
       " 'pw2',\n",
       " 'doesnot',\n",
       " 'nitpicks',\n",
       " 'bloomberg',\n",
       " 'qlty',\n",
       " 'undone',\n",
       " 'slowness',\n",
       " 'branding',\n",
       " 'elarning',\n",
       " 'ithe',\n",
       " 'cups',\n",
       " '7inch',\n",
       " 'und',\n",
       " 'accountss',\n",
       " 'bookaholic',\n",
       " '???????????',\n",
       " 'saver/wallpaper',\n",
       " 'dosent',\n",
       " 'scrape',\n",
       " 'puppy',\n",
       " 'walmart',\n",
       " 'oz',\n",
       " 'recognizing',\n",
       " 'tempting',\n",
       " 'xan',\n",
       " 'hymn',\n",
       " 'asleep?',\n",
       " 'musci',\n",
       " 'kiosk',\n",
       " '35$',\n",
       " 'malfunctioning',\n",
       " 'squarish',\n",
       " 'hroat',\n",
       " 'features…dont',\n",
       " 'woof',\n",
       " '50$',\n",
       " 'cary',\n",
       " 'whever',\n",
       " 'respectable',\n",
       " 'centerpoint',\n",
       " '89%',\n",
       " 'kicked',\n",
       " 'ike',\n",
       " 'commented',\n",
       " 'j7',\n",
       " 'earned',\n",
       " 'appts',\n",
       " 'thetap',\n",
       " 'mexico',\n",
       " 'vocabs',\n",
       " 'competent',\n",
       " 'congitions',\n",
       " 'cradle',\n",
       " 'kinder',\n",
       " 'advirtized',\n",
       " 'joints/grip',\n",
       " 'wikimedia',\n",
       " 'dilemma',\n",
       " 'flixter',\n",
       " 'beachbag',\n",
       " 'curriculum',\n",
       " 'upward',\n",
       " 'afterwards',\n",
       " 'discipline',\n",
       " 'wasn’t',\n",
       " 'townhome',\n",
       " 'downplays',\n",
       " 'g3',\n",
       " 'compensates',\n",
       " 'shutdown',\n",
       " 'wasteful',\n",
       " 'dvd/streaming',\n",
       " 'mourning',\n",
       " 'miore',\n",
       " 'wag',\n",
       " 'whisper',\n",
       " '15mbps',\n",
       " 'spills',\n",
       " 'keyboard/',\n",
       " 'menial',\n",
       " 'imitates',\n",
       " 'wiz',\n",
       " 'squinting',\n",
       " 'uber',\n",
       " 'dreamed',\n",
       " 'outgoing',\n",
       " 'nonprofit',\n",
       " 'affording',\n",
       " 'limites',\n",
       " 'valueble',\n",
       " 'diction',\n",
       " 'wierd',\n",
       " 'valuegreat',\n",
       " 'activites',\n",
       " 'cited',\n",
       " 'devoured',\n",
       " 'kindlehd',\n",
       " 'central',\n",
       " 'dexterity',\n",
       " 'simplest',\n",
       " 'fav',\n",
       " 'verbal',\n",
       " 'messy',\n",
       " 'ereading',\n",
       " 'cargo',\n",
       " 'relaxation',\n",
       " 'outfit',\n",
       " 'listeda',\n",
       " 'systym',\n",
       " 'unbecoming',\n",
       " 'gir',\n",
       " 'computer/electronics',\n",
       " '4out',\n",
       " 'broadcast',\n",
       " 'optimized',\n",
       " 'batery',\n",
       " '48',\n",
       " 'efficent',\n",
       " 'littlebit',\n",
       " 'bert',\n",
       " 'prime/netflix',\n",
       " 'vga',\n",
       " 'simple4',\n",
       " 'strips',\n",
       " 'rocking',\n",
       " 'superfun',\n",
       " '$350',\n",
       " 'visually',\n",
       " 'tweaking',\n",
       " 'hoo',\n",
       " 'fuzzy',\n",
       " 'bdus',\n",
       " 'tetevision',\n",
       " 'galapagos',\n",
       " 'seals',\n",
       " 'wholeheartedly',\n",
       " 'clams',\n",
       " 'conscamera',\n",
       " 'insulted',\n",
       " 'devotee',\n",
       " 'organization',\n",
       " 'booking',\n",
       " 'sums',\n",
       " 'onother',\n",
       " 'sofar',\n",
       " 'watchers',\n",
       " 'dynamic',\n",
       " 'quirky',\n",
       " 'justification',\n",
       " 'screencasting',\n",
       " 'flixst',\n",
       " 'hasatate',\n",
       " 'alurek',\n",
       " 'inventory',\n",
       " 'inaccurate',\n",
       " 'phoenix',\n",
       " 'irrigation',\n",
       " 'accidents',\n",
       " 'noob',\n",
       " 'ding',\n",
       " 'gameplay',\n",
       " 'hmmm',\n",
       " 'fur',\n",
       " 'romance',\n",
       " 'simpliar',\n",
       " 'nationwide',\n",
       " 'medicine',\n",
       " '2013',\n",
       " 'spells',\n",
       " 'iy',\n",
       " 's8',\n",
       " 'beeen',\n",
       " 'thisome',\n",
       " 'behave',\n",
       " 'southwest',\n",
       " 'keypad',\n",
       " 'gadgetly',\n",
       " 'weathergirl',\n",
       " 'bop',\n",
       " 'journals',\n",
       " 'bloggers',\n",
       " 'may/june',\n",
       " 'controllong',\n",
       " 'heartily',\n",
       " 'flatscreen',\n",
       " 'disallow',\n",
       " 'ought',\n",
       " 'alternate',\n",
       " 'bikerider',\n",
       " 'frequency',\n",
       " '5x',\n",
       " 'onyx',\n",
       " 'freetimeunlimited',\n",
       " 'favoite',\n",
       " 'tabo',\n",
       " 'amuses',\n",
       " 'produ',\n",
       " 'laud',\n",
       " 'aloud',\n",
       " '64g',\n",
       " 'mucheveryday',\n",
       " 'snagged',\n",
       " 's**t',\n",
       " 'session',\n",
       " 'happend',\n",
       " 'guessing',\n",
       " 'impatient',\n",
       " 'neiece',\n",
       " 'snow',\n",
       " 'cooling',\n",
       " 'syllabus',\n",
       " 'girt',\n",
       " 'inputs',\n",
       " '25%',\n",
       " 'freak',\n",
       " 'nterface',\n",
       " 'untimely',\n",
       " 'undoubtedly',\n",
       " 'zigbee',\n",
       " 'outperforms',\n",
       " 'adt',\n",
       " 'memories',\n",
       " 'c1',\n",
       " 'greatdifficult',\n",
       " 'flow',\n",
       " 'unintended',\n",
       " 'fashioned',\n",
       " 'ereaders',\n",
       " 'backlite',\n",
       " 'view/play',\n",
       " 'mass',\n",
       " 'toying',\n",
       " 'disperses',\n",
       " 'techonology',\n",
       " 'knockoff/no',\n",
       " '>iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii',\n",
       " 'time…',\n",
       " 'drawn',\n",
       " 'moto',\n",
       " 'packers',\n",
       " 'vid',\n",
       " 'grey',\n",
       " 'instantaneous',\n",
       " 'al?',\n",
       " 'procedure',\n",
       " 'individually',\n",
       " 'fm',\n",
       " 'mouse/keboard',\n",
       " 'traveler',\n",
       " 'alongst',\n",
       " 'suite',\n",
       " 'clock/radio?',\n",
       " 'beck',\n",
       " 'programmes',\n",
       " 'lightweightness',\n",
       " 'doubts',\n",
       " 'yopu',\n",
       " 'programmings',\n",
       " 'restaurant',\n",
       " 'remodel',\n",
       " 'rolls',\n",
       " 'intimidated',\n",
       " 'active/or',\n",
       " 'cpu/gpu',\n",
       " 'fr',\n",
       " 'insanely',\n",
       " 'techshe',\n",
       " 'albeit',\n",
       " 'cramps',\n",
       " 'roommate',\n",
       " 'dang',\n",
       " 'cnbc',\n",
       " 'sweden',\n",
       " 'bookworms',\n",
       " 'allotted',\n",
       " 'presentation',\n",
       " 'seasin',\n",
       " 'appli',\n",
       " 'longevity',\n",
       " 'retire',\n",
       " 'thoughtnwould',\n",
       " 'arent?',\n",
       " 'shockingly',\n",
       " 'attitude',\n",
       " 'finance',\n",
       " 'fianc',\n",
       " 'response/reaction',\n",
       " 'amnazon',\n",
       " 'purchashed',\n",
       " 'walls',\n",
       " 'qualify',\n",
       " 'ram/memory',\n",
       " 'delta',\n",
       " 'screenlike',\n",
       " 'relieve',\n",
       " 'thereafter',\n",
       " 'airline',\n",
       " 'combining',\n",
       " 'mobi',\n",
       " 'comnect',\n",
       " 'unproductive',\n",
       " 'firei',\n",
       " 'greats',\n",
       " 'priceless',\n",
       " '219',\n",
       " 'mis',\n",
       " 'u450',\n",
       " 'strides',\n",
       " 'shortest',\n",
       " 'fidgety',\n",
       " 'dissappointing',\n",
       " 'partnership',\n",
       " 'recorded',\n",
       " 'lifewould',\n",
       " 'hint',\n",
       " 'sideloading',\n",
       " 'stockers',\n",
       " 'thanked',\n",
       " 'mights',\n",
       " 'addictions',\n",
       " 'cracking',\n",
       " 'installs',\n",
       " 'perfetly',\n",
       " 'thatbi',\n",
       " 'launching',\n",
       " 'ballgame',\n",
       " 'standards',\n",
       " 'weather/morning',\n",
       " 'memeber',\n",
       " 'thisdevice',\n",
       " '12gb',\n",
       " 'fluctuation',\n",
       " 'questions/commands',\n",
       " 'doorbuster',\n",
       " 'shortcuts',\n",
       " 'platforms',\n",
       " 'casting',\n",
       " 'watcher',\n",
       " 'myriad',\n",
       " 'wrap',\n",
       " 'tanks',\n",
       " 'sis',\n",
       " 'aftermarket',\n",
       " 'masses',\n",
       " 'intern',\n",
       " 'strolling',\n",
       " 'diss',\n",
       " 'libarary',\n",
       " 'scaling',\n",
       " 'gray',\n",
       " 'commerce',\n",
       " 'cellphone',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see which words we removed\n",
    "print(len(removed_vocab))\n",
    "removed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303\n"
     ]
    }
   ],
   "source": [
    "# Now, let's see how many words we kept\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create a bag_of_words_using_vocab function that does the same thing as\n",
    "# bag_of_words above except we remove any words that are not in the vocab set\n",
    "\n",
    "def bag_of_words_using_vocab(text, vocabulary=vocab):\n",
    "    bow = bag_of_words(text)\n",
    "    words = list(bow.keys())\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            bow.pop(word)\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use bag_of_words_using_vocab to create new bows\n",
    "better_bows = []\n",
    "\n",
    "for text in text_vals:\n",
    "    better_bow = bag_of_words_using_vocab(text, vocabulary=vocab)\n",
    "    better_bows.append(better_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'this': 1,\n",
       "         'product': 1,\n",
       "         'so': 1,\n",
       "         'far': 1,\n",
       "         'has': 1,\n",
       "         'not': 1,\n",
       "         'disappointed': 1,\n",
       "         '': 2,\n",
       "         'my': 1,\n",
       "         'children': 1,\n",
       "         'love': 1,\n",
       "         'to': 2,\n",
       "         'use': 1,\n",
       "         'it': 1,\n",
       "         'and': 1,\n",
       "         'i': 1,\n",
       "         'like': 1,\n",
       "         'the': 1,\n",
       "         'ability': 1,\n",
       "         'monitor': 1,\n",
       "         'control': 1,\n",
       "         'what': 1,\n",
       "         'content': 1,\n",
       "         'they': 1,\n",
       "         'see': 1,\n",
       "         'with': 1,\n",
       "         'ease': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at what's left in the bag of words now\n",
    "better_bows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<34622x3303 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 868077 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, lets create vectors out of these new bags of words\n",
    "\n",
    "# We now have these as our X vectors\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(better_bows)\n",
    "\n",
    "# This is not a DataFrame, so we can't do X.head()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_stars = df[\"stars\"].values\n",
    "# This is not a DataFrame, so we can't do y_stars.head()\n",
    "y_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'negative', 'negative',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "\n",
    "for stars in y_stars:\n",
    "    if stars >= 4:\n",
    "        y.append(\"positive\")\n",
    "    elif stars == 3:\n",
    "        y.append(\"neutral\")\n",
    "    else:\n",
    "        y.append(\"negative\")\n",
    "\n",
    "# This is not a DataFrame, so we can't do y.head()\n",
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split into test, train\n",
    "# We want 70% train, 30% test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Estimators\n",
    "\n",
    "Use these two algorithms:\n",
    "- [Neural Network](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "- [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "__DO NOT DO CROSS VALIDATION__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 91.95%\n",
      "Test accuracy: 90.52%\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "train_acc = mnb.score(train_X, train_y)\n",
    "test_acc = mnb.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.48%\n",
      "Test accuracy: 93.24%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "train_acc = rf.score(train_X, train_y)\n",
    "test_acc = rf.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.99%\n",
      "Test accuracy: 92.99%\n",
      "CPU times: user 50 s, sys: 18.2 s, total: 1min 8s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn = MLPClassifier(random_state=seed)\n",
    "nn.fit(train_X, train_y)\n",
    "\n",
    "train_acc = nn.score(train_X, train_y)\n",
    "test_acc = nn.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation\n",
    "\n",
    "Evaluate how well we did using Confusion matrices and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Improve the Model\n",
    "\n",
    "Try these approaches to improve your model:\n",
    "- Try changing the minimum occurences we want to have to something else, see if results improve\n",
    "- Try removing useless words. Call these words \"stop-words\" in a list and prune them out of the vocab.\n",
    "  - Ex: \"the\", \"me\", \"you\", \"a\"\n",
    "- Try a different model and see if your accuracy improves (you can use CV at this point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a helper function that accepts min_occurence and stop_words\n",
    "# and produces a new vocabulary\n",
    "def create_vocab(min_occurence, stop_words):\n",
    "    # Now, let's filter out the words that have < 30 occurences\n",
    "    vocab = set()\n",
    "    removed_vocab = set()\n",
    "\n",
    "    for word, count in word_occurences.items():\n",
    "        if  count >= min_occurence and word not in stop_words:\n",
    "            vocab.add(word)\n",
    "        else:\n",
    "            removed_vocab.add(word)\n",
    "            \n",
    "    return vocab, removed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll create a helper function to take care of making new data splits\n",
    "def use_new_vocab(vocabulary):\n",
    "    new_bows = []\n",
    "\n",
    "    for text in text_vals:\n",
    "        # Use bag_of_words_using_vocab with the vocabulary\n",
    "        new_bow = bag_of_words_using_vocab(text, vocabulary=vocabulary)\n",
    "        new_bows.append(new_bow)\n",
    "    \n",
    "    # Now, creater vectors from these dictionaries\n",
    "    X = DictVectorizer().fit_transform(new_bows)\n",
    "    \n",
    "    # Create test/train splits\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=seed)\n",
    "    return (train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873\n",
      "13223\n"
     ]
    }
   ],
   "source": [
    "# Trial 1\n",
    "min_occurence = 30 # Remove all words from vocab that occur < 30 times\n",
    "stop_words = [\"\"] # Additional words to remove from vocab\n",
    "\n",
    "vocab, removed_vocab = create_vocab(min_occurence, stop_words)\n",
    "print(len(vocab))\n",
    "print(len(removed_vocab))\n",
    "\n",
    "# Create test/train splits\n",
    "train_X, test_X, train_y, test_y = use_new_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1873)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify that the shape of each vector in train_X is (1, len(vocab))\n",
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 91.25%\n",
      "Test accuracy: 90.04%\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "train_acc = mnb.score(train_X, train_y)\n",
    "test_acc = mnb.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.53%\n",
      "Test accuracy: 93.12%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "train_acc = rf.score(train_X, train_y)\n",
    "test_acc = rf.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.00%\n",
      "Test accuracy: 92.87%\n",
      "CPU times: user 32.8 s, sys: 3.66 s, total: 36.5 s\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = MLPClassifier(random_state=seed)\n",
    "nn.fit(train_X, train_y)\n",
    "\n",
    "train_acc = nn.score(train_X, train_y)\n",
    "test_acc = nn.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660\n",
      "12436\n"
     ]
    }
   ],
   "source": [
    "# Trial 2\n",
    "min_occurence = 15\n",
    "stop_words = [\"\", \"the\"] \n",
    "\n",
    "vocab, removed_vocab = create_vocab(min_occurence, stop_words)\n",
    "print(len(vocab))\n",
    "print(len(removed_vocab))\n",
    "\n",
    "# Create test/train splits\n",
    "train_X, test_X, train_y, test_y = use_new_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2660)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify that the shape of each vector in train_X is (1, len(vocab))\n",
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 91.64%\n",
      "Test accuracy: 90.36%\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_X, train_y)\n",
    "\n",
    "train_acc = mnb.score(train_X, train_y)\n",
    "test_acc = mnb.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.51%\n",
      "Test accuracy: 93.14%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=seed)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "train_acc = rf.score(train_X, train_y)\n",
    "test_acc = rf.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.00%\n",
      "Test accuracy: 92.94%\n",
      "CPU times: user 45.2 s, sys: 1.3 s, total: 46.5 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = MLPClassifier(random_state=seed)\n",
    "nn.fit(train_X, train_y)\n",
    "\n",
    "train_acc = nn.score(train_X, train_y)\n",
    "test_acc = nn.score(test_X, test_y)\n",
    "\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
